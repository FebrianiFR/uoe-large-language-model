{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Outline {.smaller}\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "### Part 1-2\n",
        "1. **Foundations of LLMs**\n",
        "   - Basic concepts and applications\n",
        "   - Key components and capabilities\n",
        "\n",
        "2. **Word Representation Evolution**\n",
        "   - Classical approaches (One-hot, BoW, TF-IDF)\n",
        "   - Neural approaches (Word2Vec)\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "### Part 3-4\n",
        "3. **Advanced Architectures**\n",
        "   - RNN and its limitations\n",
        "   - Attention mechanism\n",
        "   - Transformer architecture\n",
        "\n",
        "4. **Modern LLM Landscape**\n",
        "   - BERT vs GPT architectures\n",
        "   - Current trends and applications\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Use of LLMs {transition=\"convex\"}\n",
        "\n",
        "They are trained to tackle text-based tasks:\n",
        "\n",
        "- Text classification.\n",
        "- Text summarization (including sentiment analysis).\n",
        "- Text generation (including translation and coding).\n",
        "- Questions/Answers.\n",
        "- Common sense reasoning.\n",
        "\n",
        ":::{.fragment .callout-important}\n",
        "\n",
        "LLMs (Large Language Models) are not necessarily GenAI, for instance, BERT is not a GenAI, but still an LLM.\n",
        "\n",
        ":::\n",
        "\n",
        "## Tasks that LLM can do\n",
        "\n",
        "![Use of LLM](imgs/L1_2.png)\n",
        "\n",
        "## Key Elements of LLMs {.smaller}\n",
        "\n",
        "- **Tokenization & Embeddings**: Text → tokens → semantic vectors\n",
        "\n",
        "- **Attention**: Weighs input relevance for context-aware predictions\n",
        "\n",
        "- **Transformer**: Multi-layer architecture combining attention and neural networks\n",
        "\n",
        "- **Training**: General pretraining on large corpora, then task-specific fine-tuning\n",
        "\n",
        "\n",
        "## Word Embeddings\n",
        "\n",
        "- One-hot encoding\n",
        "- Bag of Words\n",
        "- TF-IDF\n",
        "- Word2Vec\n",
        "\n",
        "## One-hot encoding{transition=\"fade-in zoom-out\"}\n",
        "\n",
        "\n",
        "- Each word is uniquely represented by a vector in a high-dimension space.\n",
        "- Each dimension corresponds to one of the words in the vocabulary.\n",
        "\n",
        "\n",
        "## Mathematical Intuition: Step by Step {.smaller}\n",
        "\n",
        "### 1. Word Embeddings\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "From one-hot vector to embedding:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{word}_{\\text{one-hot}} &= [0,1,0,0,\\dots,0] \\\\\n",
        "\\text{embedding} &= W \\cdot \\text{word}_{\\text{one-hot}} \\\\\n",
        "&= \\text{corresponding row in }W\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        ":::{.callout-tip}\n",
        "This is why we can directly look up the embedding instead of doing matrix multiplication!\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Coding Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code-block-font-size": "1.4em",
        "output-location": "column"
      },
      "outputs": [],
      "source": [
        "#| code-summary: One-hot Encoding Example\n",
        "#| code-line-numbers: '7'\n",
        "#| echo: true\n",
        "# Plot the 3D space\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "vocab = ['I', 'love', 'apple']\n",
        "# Define a simple vocabulary of three words\n",
        "vocab_size = len(vocab)\n",
        "# Create a mapping from words to indices\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Function to one-hot encode a word\n",
        "def one_hot_encode(word, vocab_size):\n",
        "    one_hot_vector = np.zeros(vocab_size)\n",
        "    index = word_to_index[word]\n",
        "    one_hot_vector[index] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "# Create one-hot encoded vectors for the simple vocabulary\n",
        "one_hot_vectors = np.array([one_hot_encode(word, vocab_size) for word in vocab])\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8), facecolor='none',edgecolor='none')\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Origin point\n",
        "origin = [0, 0, 0]\n",
        "\n",
        "# Define colors for each word\n",
        "colors = ['red', 'green', 'blue']\n",
        "\n",
        "# Add arrows and labels to the 3D plot with different colors\n",
        "for i, word in enumerate(vocab):\n",
        "    ax.quiver(*origin, *one_hot_vectors[i], length=1, arrow_length_ratio=0.1, color=colors[i], label=word)\n",
        "    ax.text(one_hot_vectors[i, 0], one_hot_vectors[i, 1], one_hot_vectors[i, 2], word, color=colors[i], fontsize=12)\n",
        "\n",
        "# Set the scale of the x, y, and z axes\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_zlim([0, 1])\n",
        "\n",
        "ax.set_title(\"One-Hot Encoding Visualization in 3-Word Space\")\n",
        "ax.set_xlabel(\"Dimension: 'apple'\")\n",
        "ax.set_ylabel(\"Dimension: 'banana'\")\n",
        "ax.set_zlabel(\"Dimension: 'grape'\")\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Any Drawbacks?\n",
        "::: {.incremental}\n",
        "\n",
        "- High dimensionality for large vocabularies\n",
        "\n",
        "- Inability to represent word meanings or similarities.\n",
        ":::\n",
        "\n",
        "\n",
        "## Bag of Words\n",
        "**Characteristics:**\n",
        "\n",
        "- In BoW, a document is represented as a vector where each element corresponds to a word in the vocabulary\n",
        "- The value represents the frequency or presence of that word in the document\n",
        "\n",
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output-location": "column"
      },
      "outputs": [],
      "source": [
        "#| code-summary: Bag of Words Example\n",
        "#| code-line-numbers: 8-12\n",
        "#| echo: true\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from collections import Counter\n",
        "\n",
        "# Define a vocabulary of three words\n",
        "vocab = ['I', 'like', 'apple']\n",
        "documents = [\n",
        "    \"I like apple\",         # Sentence 1\n",
        "    \"apple, I like apple!\", # Sentence 2\n",
        "    \"I, I like apple\"     # Sentence 3\n",
        "]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create a mapping from words to indices\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Example meaningful sentences using only three words\n",
        "\n",
        "# Function to build a BoW representation\n",
        "def build_bow(documents, vocab):\n",
        "    bow_matrix = np.zeros((len(documents), len(vocab)))\n",
        "    for i, doc in enumerate(documents):\n",
        "        word_counts = Counter([word.strip('.,!') for word in doc.split() if word in vocab])\n",
        "        for word, count in word_counts.items():\n",
        "            if word in word_to_index:\n",
        "                bow_matrix[i, word_to_index[word]] = count\n",
        "    return bow_matrix\n",
        "\n",
        "# Build BoW representation\n",
        "bow_matrix = build_bow(documents, vocab)\n",
        "# Plot the BoW matrix in 3D using vectors with more 3D appearance\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlim([0, 2])\n",
        "ax.set_ylim([0, 2])\n",
        "ax.set_zlim([0, 2])\n",
        "\n",
        "# Define origin\n",
        "origin = np.array([0, 0, 0])\n",
        "\n",
        "# Define colors for each vector\n",
        "colors = ['r', 'g', 'b']\n",
        "\n",
        "# Add vector and labels to the 3D plot with different colors\n",
        "# Define the correct vectors for each sentence\n",
        "correct_vectors = np.array([[1, 1, 1], [1, 1, 2], [2, 1, 1]])\n",
        "for i, doc in enumerate(documents):\n",
        "    vec_from_origin = correct_vectors[i]\n",
        "    # Draw vectors with arrowheads and make them appear more 3D\n",
        "    ax.quiver(*origin, *vec_from_origin, arrow_length_ratio=0.1, color=colors[i], label=f'Sentence {i+1}')\n",
        "    ax.text(*vec_from_origin, f'Sentence {i+1}', fontsize=12, color=colors[i])\n",
        "\n",
        "# Add gridlines for better 3D effect\n",
        "ax.view_init(azim=20)  # Rotate the plot by 90 degrees\n",
        "\n",
        "ax.grid(True)\n",
        "\n",
        "ax.set_title(\"Bag-of-Words Representation in 3-Word Space\")\n",
        "ax.set_xlabel(\"Count of 'I'\")\n",
        "ax.set_ylabel(\"Count of 'like'\")\n",
        "ax.set_zlabel(\"Count of 'apple'\")\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BOW Discussion\n",
        "\n",
        "#### Drawbacks\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "- Treats all words equally regardless of their importance\n",
        "- Some words are too common, e.g., is, are, we, he, she...\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "#### Solution\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "- Use stopwords to filter those words\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        ":::{.callout-caution}\n",
        "Howerver, this is not an algorithmic solution, as there are always words that cannot be included in stopwords list.\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## TF-IDF {.smaller}\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column}\n",
        "\n",
        "- **Term Frequency:** In document d, the frequency represents the number of instances of a given word t.\n",
        "- **Document Frequency:** The number of documents in which the word is present in document set N.\n",
        "- **Inverse Document Frequency:** The more it appears, the less it becomes relevant.\n",
        "\n",
        ":::\n",
        ":::{.column}\n",
        "\n",
        "![](imgs/L1_8.jpg)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## TF-IDF: an Illustration\n",
        "\n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        "\n",
        "- Document 1:\n",
        "  - [I love apple]{style=\"color: green\"}.\n",
        "- Document 2:\n",
        "  - [I love banana]{style=\"color: green\"}.\n",
        "- Document 3:\n",
        "  - [You love grape]{style=\"color: green\"}.\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"60%\"}\n",
        "\n",
        "| Term   | TF  | IDF | TF-IDF |\n",
        "|--------|-----|-----|--------|\n",
        "| I      | 1/3 | 3/2 | 1/2    |\n",
        "| love   | 1/3 | 1   | 1/3    |\n",
        "| apple  | 1/3 | 3   | 1      |\n",
        "| banana | 0   | 3   | 0      |\n",
        "| you    | 0   | 3   | 0      |\n",
        "| grape  | 0   | 3   | 0      |\n",
        "\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## TF-IDF Discussion\n",
        "\n",
        ":::{.fragment}\n",
        "**Drawbacks**\n",
        "\n",
        "- Ignores word order and context,\n",
        "\n",
        "- Suffers from data sparsity.\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "Could be [One]{style=\"color: green\"} way out, that is...\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "#### Contextualised Learning\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "Let's introduce **Word2Vec**, the way to represent words in a continuous and condensed vector space.\n",
        "\n",
        "::: {.aside}\n",
        "\n",
        "Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\"*Advances in neural information processing systems* 26 (2013).\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "## How Word2Vec works\n",
        "\n",
        "\n",
        "- Each word is initially one-hot encoded.\n",
        "- Then using CBOW/Skip-gram to train the model.\n",
        "- CBOW uses the context to predict the word;\n",
        "- Skip-gram uses the word to predict the context.\n",
        "\n",
        "![](imgs/L1_4.png)\n",
        "\n",
        "\n",
        "\n",
        "## A Simple Example\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column}\n",
        "\n",
        "- Tong poured himself a cup of coffee.\n",
        "- Window_size = 4\n",
        "\n",
        ":::\n",
        ":::{.column}\n",
        "\n",
        "**Continuous Bag-of-Words**\n",
        "\n",
        "- **Input:** [Tong, poured, a, cup]{style=\"color: green\"}\n",
        "- **Output:** himself\n",
        "\n",
        "**Skip-Gram Model**\n",
        "\n",
        "- **Input:** himself\n",
        "- **Output:** [Tong, poured, a, cup]{style=\"color: green\"}\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Word Embeddings {auto-animation=true}\n",
        "\n",
        ":::{.columns}\n",
        "\n",
        ":::{.column width=\"60%\"}\n",
        "![](imgs/L1_5.png)\n",
        ":::\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        "### Key Concepts\n",
        "- Words with similar meanings cluster together\n",
        "- Relationships between words are preserved\n",
        "  - King - Man + Woman ≈ Queen\n",
        "  - Paris - France + Italy ≈ Rome\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Understanding Word Vectors {.smaller}\n",
        "\n",
        "Consider these word relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code-block-font-size": "2em"
      },
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "# Word vector examples\n",
        "king  = [0.0, 0.7, 0.3, 0.9]\n",
        "man   = [0.1, 0.5, 0.2, 0.3]\n",
        "woman = [0.1, 0.4, 0.2, 0.8]\n",
        "queen = [0.0, 0.6, 0.3, 1.4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[The relationship is preserved!]{.fragment .highlight-blue}\n",
        "\n",
        ":::{.callout-note}\n",
        "### Vector Math\n",
        "$\\vec{king} - \\vec{man} + \\vec{woman} \\approx \\vec{queen}$\n",
        "\n",
        "This shows how word vectors capture:\n",
        "\n",
        "- Gender relationships\n",
        "- Royal status\n",
        "- Semantic meaning\n",
        ":::\n",
        "\n",
        "\n",
        "## Are there still drawbacks?\n",
        "\n",
        ":::{.incremental}\n",
        "\n",
        "- Static word representations\n",
        "- Inability to handle polysemy (multiple meanings of a word)\n",
        "- still lack of contextualization\n",
        "\n",
        ":::\n",
        "\n",
        "## Contextualised Embeddings {.smaller}\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "We want:\n",
        "\n",
        "- Learns word representations that are sensitive to the context in which the words appear;\n",
        "- Embeddings should be a function of the entire input sentence, allowing for dynamic and context-dependent representations.\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "For example:\n",
        "\n",
        "*I love apple products.* How to interpret the meaning of this sentence?\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "- I love apple products, they are delicious.\n",
        "- I love apple products, they are easy to use.\n",
        "\n",
        ":::\n",
        "\n",
        "## That's why we need Transformers!\n",
        "\n",
        "\n",
        "\n",
        "![](imgs/L1_6.png)\n",
        "\n",
        "\n",
        "\n",
        "## That's why we need Transformers! {transition=\"zoom\" visibility=\"uncounted\"}\n",
        "\n",
        "\n",
        "\n",
        "![](imgs/L1_3.png)\n",
        "\n",
        "\n",
        "## The Origin of Transformer\n",
        "\n",
        "- Recurrent Neural Networks\n",
        "\n",
        "- Encoder-decoder infrastructure\n",
        "\n",
        "- Self-Attention\n",
        "\n",
        "- Attention is all your need\n",
        "\n",
        "- What is the Future? Google vs OpenAI\n",
        "\n",
        "\n",
        "## Recurrent Neural Network {.smaller}\n",
        "\n",
        "- At each time step, the hidden state serves as the \"memory\" of the network and is updated and passed along, allowing the model to capture long-term dependencies in the sequence. This property makes RNNs particularly well-suited for processing sequential data such as text or speech.\n",
        "\n",
        "![](imgs/L1_7.gif){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## RNN Gradient Flow: Visualised {.smaller}\n",
        "\n",
        "Let's understand why gradients vanish in RNNs:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "1. **Basic RNN Update**\n",
        "   $$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n",
        "\n",
        "2. **Gradient Flow (Chain Rule)**\n",
        "   $$\\frac{\\partial h_t}{\\partial h_{t-k}} = \\prod_{i=1}^k \\frac{\\partial h_{t-i+1}}{\\partial h_{t-i}}$$\n",
        "\n",
        "3. **Each Step's Gradient**\n",
        "   $$\\frac{\\partial h_{t}}{\\partial h_{t-1}} = \\text{diag}(1-\\tanh^2(z_t))W_h$$\n",
        ":::\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        ":::{.callout-warning}\n",
        "### Problem Visualisation\n",
        "![](imgs/gradient_flow.png)\n",
        "\n",
        "As k increases, gradient either:\n",
        "\n",
        "- [Vanishes]{style=\"color: blue\"} (most common)\n",
        "- [Explodes]{style=\"color: red\"} (rare)\n",
        "\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Understanding Gradient Terms {.smaller}\n",
        "\n",
        ":::{.callout-note}\n",
        "### Breaking Down the Gradient\n",
        "1. $\\text{diag}(1-\\tanh^2(z_t))$:\n",
        "   - This is the derivative of tanh\n",
        "   - Always ≤ 1 in magnitude\n",
        "   - Gets very small when $z_t$ is large\n",
        "\n",
        "2. Multiplication by $W_h$:\n",
        "   - Small eigenvalues cause vanishing\n",
        "   - Large eigenvalues cause explosion\n",
        "   - Example: with 100 steps\n",
        "     - If each step × 0.9: $0.9^{100} \\approx 0$\n",
        "     - If each step × 1.1: $1.1^{100} \\approx 13,781$\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Consequences{.smaller}\n",
        "\n",
        "During Backpropagation Through Time, the gradient of the loss with respect to a weight $W_h$ is affected by the gradient of the loss with respect to each hidden state $h_t$. The chain rule provides the mechanism for computing this relationship:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_h}\n",
        "$$\n",
        "\n",
        "Here, $\\frac{\\partial h_t}{\\partial W_h}$ can be expanded through the chain of derivatives relating subsequent hidden states:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial h_t}{\\partial W_h} = \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W_h} = \\dots = \\prod_{k=1}^{t} \\frac{\\partial h_k}{\\partial h_{k-1}} \\frac{\\partial h_1}{\\partial W_h}\n",
        "$$\n",
        "\n",
        ":::{.callout-warning .fragment}\n",
        "\n",
        "The weights associated with early inputs hardly update!\n",
        "\n",
        ":::\n",
        "\n",
        "## Beyond RNNs {.smaller}\n",
        "\n",
        ":::{.incremental}\n",
        "- RNNs struggle with long sequences due to gradient problems\n",
        "- We need a better way to:\n",
        "  1. Process long sequences effectively\n",
        "  2. Capture dependencies regardless of distance\n",
        "  3. Allow parallel processing\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "This leads us to explore a different architecture...\n",
        ":::\n",
        "\n",
        "## Encoder-decoder Infrastructure {.smaller}\n",
        "\n",
        "**Encoder:** receive input data and convert it into a fixed internal representation.\n",
        "\n",
        "- Encoder is good at understanding the input and generating a meaningful, dense representation.\n",
        "- Without the need to generate sequences, these models can be less complex and faster at inference for tasks.\n",
        "\n",
        "**Decoder:** transform the context vector produced by the encoder into the target output.\n",
        "\n",
        "- Decoder is good at sequence generation and directly mapping the input to output.\n",
        "- Their performance heavily depends on the quality and relevance of the input prompt.\n",
        "- Relatively weaker in understanding long-range textual input.\n",
        "\n",
        "## Encoder-decoder Infrastructure: An Illustration\n",
        "\n",
        "![](imgs/L1_9.png)\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "**Drawbacks:**\n",
        "The encoding step needs to represent the entire input sequence as a single vector, which can cause information loss due to compression.\n",
        "\n",
        ":::\n",
        "\n",
        "## We Need Long-range Memory\n",
        "\n",
        "\n",
        "- Previous information be represented in various channels;\n",
        "- Appropriate weight could be assigned to filter out proper information;\n",
        "- Dynamic adjustment of these weights based on context;\n",
        "- Capability to handle long dependencies;\n",
        "- Efficient computation and memory usage;\n",
        "\n",
        "## Long-range Memory {visibility=\"uncounted\"}\n",
        "\n",
        "![](imgs/L1_10.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Attention Mechanism: An Intuitive View {.smaller}\n",
        "\n",
        "Think of reading a complex novel:\n",
        "\n",
        ":::{.incremental}\n",
        "- When you read \"The old man bought **it** at the store\", your brain automatically:\n",
        "  1. Looks back at previous context to understand what \"it\" refers to\n",
        "  2. Focuses more on relevant words and less on others\n",
        "  3. Combines multiple pieces of information to make sense\n",
        ":::\n",
        "\n",
        ":::{.fragment}\n",
        "This is exactly how attention works in LLMs!\n",
        ":::\n",
        "\n",
        "## Attention Mechanism: Basic Structure {.smaller}\n",
        "\n",
        "Think of attention as a smart way to focus on important information:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "![](imgs/L1_12.png){fig-align=\"center\"}\n",
        ":::\n",
        ":::{.column width=\"40%\"}\n",
        "### How It Works\n",
        "1. For each word, we:\n",
        "   - Look at all other words\n",
        "   - Decide which ones are relevant\n",
        "   - Combine their information\n",
        "\n",
        ":::{.fragment}\n",
        "Just like how you focus on key words when reading!\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Attention in Practice {.smaller}\n",
        "\n",
        "### Example 1: Language Translation\n",
        "```text\n",
        "English: \"The bank by the river is steep\"\n",
        "French:  \"La rive près de la rivière est escarpée\"\n",
        "```\n",
        "\n",
        ":::{.fragment}\n",
        "Here, \"bank\" could mean:\n",
        "- A financial institution (banque)\n",
        "- A riverbank (rive)\n",
        "\n",
        "Attention helps the model focus on \"river\" to choose the correct translation!\n",
        ":::\n",
        "\n",
        "### Example 2: Question Answering\n",
        "```text\n",
        "Context: \"Marie Curie won the Nobel Prize in Physics in 1903\n",
        "         and in Chemistry in 1911\"\n",
        "Question: \"When did Curie win the Chemistry prize?\"\n",
        "```\n",
        "\n",
        ":::{.fragment}\n",
        "The attention mechanism will:\n",
        "1. Focus heavily on \"1911\" and \"Chemistry\"\n",
        "2. Pay less attention to \"1903\" and \"Physics\"\n",
        "3. Connect \"Curie\" with the relevant date\n",
        ":::\n",
        "\n",
        "## Attention Visualization {.smaller}\n",
        "\n",
        "Let's see how attention works in sentiment analysis:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "*\"The movie was terrible, but the actor's performance was brilliant\"*\n",
        "\n",
        "![](imgs/L1_attention_example.png)\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "### How Attention Works Here:\n",
        "1. Each word pays attention to other relevant words\n",
        "2. Sentiment words (\"terrible\", \"brilliant\") get high attention\n",
        "3. Connections show which words influence each other\n",
        ":::\n",
        ":::\n",
        "\n",
        "## How to calculate the Attention Score?\n",
        "$$\n",
        " \\text{Attention Scores} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "What are $Q$ $K$ and $V$? Why do they matter?\n",
        "\n",
        "They are very important engineer-wise. Mathematically they are linear transformations.\n",
        "\n",
        ":::{.callout-note}\n",
        "### Softmax Function\n",
        "For example, if we have scores [2.0, 1.0, 0.1] and we want to apply softmax, we get:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{softmax}(2.0) &= \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.1}} \\approx 0.7 \\\\\n",
        "\\text{softmax}(1.0) &= \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.1}} \\approx 0.2 \\\\\n",
        "\\text{softmax}(0.1) &= \\frac{e^{0.1}}{e^{2.0} + e^{1.0} + e^{0.1}} \\approx 0.1\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## A Bit Linear Algebra {.smaller}\n",
        "\n",
        "Image you have a word embeddings matrix $X$:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{\"I\"} & : \\begin{bmatrix} 1 & 2 & 1 & 2 & 1 \\end{bmatrix} \\\\\n",
        "\\text{\"love\"} & : \\begin{bmatrix} 1 & 1 & 3 & 2 & 1 \\end{bmatrix} \\\\\n",
        "\\text{\"apple\"} & : \\begin{bmatrix} 3 & 1 & 2 & 1 & 1 \\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        ":::{.callout-note .fragment}\n",
        "\n",
        "What do these numbers mean?\n",
        "\n",
        "- Each word is represented by 5 numbers,\n",
        "- These numbers capture different aspects of the word,\n",
        "- Similar words should have similar patterns.\n",
        "\n",
        ":::\n",
        "\n",
        "## How Words Relate to Each Other {.smaller}\n",
        "\n",
        "What are the implications of the self dot product $X X^T$?\n",
        "\n",
        "$$\n",
        "X X^T =\n",
        "\\begin{bmatrix}\n",
        "11 & 11 & 10 \\\\\n",
        "11 & 16 & 13 \\\\\n",
        "10 & 13 & 16\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        ":::{.callout-important .fragment}\n",
        "\n",
        "- Mathematically, the dot product is a projection of one vector on the other.\n",
        "- Larger numbers = words are more relate.\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Construct the Embeddings {.smaller}\n",
        "\n",
        "That is, if two words are irrelevant, the dot product should be zero.\n",
        "\n",
        "we standardize $XX^T$ by taking a softmax function for each row:\n",
        "\n",
        "The softmax of each row in the matrix $X X^T$ is computed as follows:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(X X^T) =\n",
        "\\begin{bmatrix}\n",
        "0.422 & 0.422 & 0.155 \\\\\n",
        "0.006 & 0.947 & 0.047 \\\\\n",
        "0.002 & 0.047 & 0.950\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "W_{I\\ \\ \\ \\ \\ } \\\\\n",
        "W_{love\\ } \\\\\n",
        "W_{apple}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        ":::{.callout-important .fragment}\n",
        "\n",
        "- Each row shows how much a word \"pays attention\" to other words,\n",
        "- \"love\" and \"apple\" mostly focus on themselves (95%),\n",
        "- \"I\" splits attention between itself and \"love\"\n",
        "\n",
        ":::\n",
        "\n",
        "## Construct the Embeddings {.smaller}\n",
        "\n",
        "$$X^N = Softmax(XX^T)X$$\n",
        "\n",
        "$$X^N_{I} = W_I *X = \\begin{bmatrix} 1.4 & 1.4 & 2 & 1.8 & 1 \\end{bmatrix}$$\n",
        "\n",
        "In this new vector, each dimension's value is derived from a weighted sum of the values in that dimension across three word vectors.\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"40%\"}\n",
        "\n",
        ":::{.callout-note}\n",
        "For \"I\", the new meaning combines:\n",
        "\n",
        "- 42% of \"I\"'s original meaning\n",
        "- 42% of \"love\"'s meaning\n",
        "- 16% of \"apple\"'s meaning\n",
        "\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"60%\"}\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "- This is how words learn from context!\n",
        "- Words get influenced by related words\n",
        "- Context helps disambiguate meaning\n",
        "- Similar to how we understand words in sentences\n",
        "\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Query, Key and Value {.smaller}\n",
        "\n",
        "The attention mechanism has two main variants:\n",
        "\n",
        "### 1. Self-Attention\n",
        "When the sequence attends to itself (e.g., understanding a sentence):\n",
        "$$\n",
        "\\begin{align*}\n",
        "Q &= X W_Q \\\\\n",
        "K &= X W_K \\\\\n",
        "V &= X W_V\n",
        "\\end{align*}\n",
        "$$\n",
        "Where X is the input sequence.\n",
        "\n",
        "### 2. Cross-Attention\n",
        "When one sequence attends to another (e.g., translation from source to target):\n",
        "$$\n",
        "\\begin{align*}\n",
        "Q &= X_{target} W_Q & \\text{(e.g., French sentence)} \\\\\n",
        "K &= X_{source} W_K & \\text{(e.g., English sentence)} \\\\\n",
        "V &= X_{source} W_V & \\text{(same as key sequence)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## How to calculate the Attention Score? {.smaller}\n",
        "\n",
        "In both cases, the attention scores are calculated the same way:\n",
        "$$\n",
        " \\text{Attention Scores} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "Simple calculation shows:\n",
        "\n",
        "$$\n",
        "QK^T = X (W_Q W_K^T)\n",
        "X^T$$\n",
        "\n",
        "It is still a projection, just with additional complexity.\n",
        "\n",
        "### What is the $\\sqrt{d_k}$ for?\n",
        "\n",
        "- Assume that elements in $Q$ and $K$ are normalised to $N(0,1)$. Then the elements in $QK^T$ will have a mean of 0 and a variance of $d$. As $d$ becomes very large, the variance of the elements will also become very large.\n",
        "\n",
        "- Therefore, by dividing each element in $\\sqrt{d}$, the variance is brought back to 1.\n",
        "\n",
        "- Finally, we manage to keep the gradient values stable during the training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Cross-Attention Example {.smaller}\n",
        "\n",
        "Let's see how cross-attention works in translation:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"40%\"}\n",
        "\n",
        "- English: [\"I\", \"love\", \"ML\"]\n",
        "- French: [\"Je\"]\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Q = \\begin{bmatrix}\n",
        "q_{Je}\n",
        "\\end{bmatrix} \\\\\n",
        "K = \\begin{bmatrix}\n",
        "k_I \\\\ k_{love} \\\\ k_{ML}\n",
        "\\end{bmatrix}\n",
        "V = \\begin{bmatrix}\n",
        "v_I \\\\ v_{love} \\\\ v_{ML}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::{.column width=\"60%\"}\n",
        "\n",
        "### After Computation:\n",
        "$$\\text{softmax}(QK^T) = \\begin{bmatrix}\n",
        "0.8 & 0.15 & 0.05\n",
        "\\end{bmatrix}$$\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "When generating \"Je\":\n",
        "\n",
        "- 80% attention to \"I\"\n",
        "- 15% attention to \"love\"\n",
        "- 5% attention to \"ML\"\n",
        "\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "In encoder-decoder architectures like machine translation,Encoder uses self-attention to understand the source sentence, while decoder uses both self-attention (on what it's generated) and cross-attention (to look at source).\n",
        "\n",
        "\n",
        "\n",
        "## Summaries {.smaller}\n",
        "\n",
        "Let's break it down step by step:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"60%\"}\n",
        "### Step 1: Calculate Attention Scores\n",
        "$$[\\color{#2780e3}{QK^T}] = \\begin{bmatrix}\n",
        "q_1 \\cdot k_1 & q_1 \\cdot k_2 \\\\\n",
        "q_2 \\cdot k_1 & q_2 \\cdot k_2\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "### Step 2: Scale the Scores\n",
        "$$[\\color{#3fb618}{\\frac{QK^T}{\\sqrt{d_k}}}]$$\n",
        ":::\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        "### Why Scale?\n",
        "- Prevents extremely large values\n",
        "- Keeps gradients stable\n",
        "- Makes training smoother\n",
        "\n",
        ":::{.callout-tip}\n",
        "Like normalizing your test scores to be between 0 and 1\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Computing Attention {.smaller}\n",
        "\n",
        "Let's see a concrete example:\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "Input: [\"I\", \"love\", \"ML\"]\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "Q &= \\begin{bmatrix}\n",
        "q_1 \\\\ q_2 \\\\ q_3\n",
        "\\end{bmatrix} \\\\\n",
        "K &= \\begin{bmatrix}\n",
        "k_1 \\\\ k_2 \\\\ k_3\n",
        "\\end{bmatrix} \\\\\n",
        "V &= \\begin{bmatrix}\n",
        "v_1 \\\\ v_2 \\\\ v_3\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "### After Computation:\n",
        "$$\\text{softmax}(QK^T) = \\begin{bmatrix}\n",
        "0.7 & 0.2 & 0.1 \\\\\n",
        "0.1 & 0.8 & 0.1 \\\\\n",
        "0.1 & 0.2 & 0.7\n",
        "\\end{bmatrix}$$\n",
        "\n",
        ":::{.callout-note}\n",
        "Each row shows how much attention each word pays to others:\n",
        "\n",
        "- \"I\" pays 70% attention to itself\n",
        "- \"love\" pays 80% attention to itself\n",
        "- \"ML\" pays 70% attention to itself\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Why Attention Works {.smaller}\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "### Key Benefits\n",
        "1. Direct connections between words\n",
        "2. No information bottleneck\n",
        "3. Parallel computation\n",
        "4. Better gradient flow\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "### Compared to RNNs\n",
        "\n",
        "- No vanishing gradients\n",
        "- Can handle long sequences\n",
        "- Faster training\n",
        ":::\n",
        ":::\n",
        "\n",
        ":::{.fragment .callout-tip}\n",
        "Think of attention as a [smart search system]{style=\"color: #2780e3\"} that knows what's [important]{style=\"color: #3fb618\"} and returns [relevant information]{style=\"color: #e83e8c\"}\n",
        ":::\n",
        "\n",
        "## Self-Attention\n",
        "\n",
        "Attenion mechanism maps input information to output, this is called **cross-attention.** However, when we read books, it is nature for us to pay attention to previous keywords and use them to help understanding the subsequent context.\n",
        "\n",
        "- In a traditional attention mechanism, it is normal that $Q$,$K$ and $V$ are from different sources, $Q$ might from the translated language, $K$ might from the original language.\n",
        "- In a self-attention mechanism, $Q$,$K$ and $V$ are trained from the same sources.\n",
        "\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "Let's look at an example:\n",
        "\n",
        "*The animal didn't cross the street because it was too tired.*\n",
        "\n",
        "![](imgs/L1_15.png){fig-align=\"center\"}\n",
        "\n",
        ":::{.callout-tip .fragment}\n",
        "\n",
        "This implies that 'it' should refers to the word 'animal'.\n",
        "\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "if we add two attention heads, i.e., two independent attention mechanism/matrices to screen this sentence:\n",
        "\n",
        "![](imgs/L1_16.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        ":::{.callout-tip .fragment}\n",
        "\n",
        "'it' related to 'animal' and 'tired'. **because of the word 'tired', 'it' refers to 'animal'.\n",
        "\n",
        ":::\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "How about another sentence: *The animal didn't cross the street because it was too wide.*\n",
        "\n",
        "Have a try here: [Attn Viz Tool](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=OJKU36QAfqOC)\n",
        "\n",
        "\n",
        "## One More Question\n",
        "\n",
        "As attention layer signficantly improves the performance. It raises a question:\n",
        "\n",
        ":::{.fragment style=\"color: red; text-align: center\"}\n",
        "\n",
        "**Why we need RNN/CNN/LSTM/...?**\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## Naturally... {.center visibility=\"uncounted\"}\n",
        "\n",
        "\n",
        "\n",
        "## Attention is all your need\n",
        "\n",
        ":::{.fragment}\n",
        "\n",
        "![](imgs/L1_11.png){fig-align=\"center\"}\n",
        "\n",
        ":::\n",
        "\n",
        "## Google Vs OpenAI{transition=\"zoom\"}\n",
        "\n",
        "![](imgs/L1_13.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## The Birth of GPT {.smaller}\n",
        "\n",
        "- The GPT-1 architecture was a twelve-layer decoder-only transformer, using twelve masked self-attention heads, with 64-dimensional states each (for a total of 768).\n",
        "- https://github.com/openai/finetune-transformer-lm\n",
        "- Radford, Alec, et al. \"Improving language understanding by generative pre-training.\" (2018)\n",
        "\n",
        "\n",
        "![](imgs/L1_14.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "## BERT VS GPT {.smaller}\n",
        "\n",
        "|       | GPT | BERT |\n",
        "|-|-----|------|\n",
        "| Pros  | - Excels in generating coherent text.<br>- Autoregressive nature suitable for dialogue systems.<br>- Simpler training process compared to bidirectional models. | - Bidirectional context enhances understanding.<br>- Versatile across various NLP tasks.<br>- Adaptable for specific challenges with minimal additional training. |\n",
        "| Cons  | - Limited contextual understanding compared to BERT.<br>- Less flexible for tasks requiring deep understanding of context. | - Computationally expensive, requiring substantial resources.<br>- Can overfit on smaller datasets. |\n",
        "\n",
        "\n",
        "## BERT vs GPT: Real-world Applications {.smaller}\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "### BERT Applications\n",
        "- **Document Classification**\n",
        "  - Email spam detection\n",
        "  - News categorization\n",
        "  - Content moderation\n",
        "\n",
        "- **Information Extraction**\n",
        "  - Resume parsing\n",
        "  - Medical record analysis\n",
        "  - Legal document review\n",
        "\n",
        "- **Question Answering**\n",
        "  - Customer service bots\n",
        "  - Educational platforms\n",
        "  - Technical documentation search\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "### GPT Applications\n",
        "- **Content Generation**\n",
        "  - Marketing copy\n",
        "  - Product descriptions\n",
        "  - Creative writing\n",
        "\n",
        "- **Conversation**\n",
        "  - Customer support chatbots\n",
        "  - Language learning assistants\n",
        "  - Virtual companions\n",
        "\n",
        "- **Code Generation**\n",
        "  - Code completion\n",
        "  - Documentation writing\n",
        "  - Bug fixing suggestions\n",
        ":::\n",
        ":::\n",
        "\n",
        "## BERT vs GPT: Practical Examples {.smaller}\n",
        "\n",
        "### BERT Example: Sentiment Analysis\n",
        "```text\n",
        "Input: \"The food was cold but the service was great\"\n",
        "BERT: Analyzes the entire sentence at once to understand:\n",
        "- \"cold\" (negative) applies to \"food\"\n",
        "- \"great\" (positive) applies to \"service\"\n",
        "Result: Mixed sentiment, context-aware understanding\n",
        "```\n",
        "\n",
        "### GPT Example: Writing Assistant\n",
        "```text\n",
        "Prompt: \"Write a product description for a new coffee maker\"\n",
        "GPT: Generates coherent, contextual text:\n",
        "\"Introducing our latest coffee maker, featuring precision temperature\n",
        "control and a built-in grinder. Start your mornings with...\"\n",
        "```\n",
        "\n",
        ":::{.fragment .callout-tip}\n",
        "Choose BERT when you need deep understanding, GPT when you need generation!\n",
        ":::\n",
        "\n",
        "## Real-world Applications {.smaller}\n",
        "\n",
        "### 1. Word Embeddings in Practice\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "#### Search Engine Enhancement\n",
        "```python\n",
        "# Example: Semantic search\n",
        "query = \"affordable laptop\"\n",
        "similar_terms = model.similar_words(query)\n",
        "# Results: \"budget computer\", \"cheap notebook\",\n",
        "#          \"inexpensive PC\"\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### Product Recommendations\n",
        "```python\n",
        "# Example: Product matching\n",
        "user_query = \"running shoes\"\n",
        "related = [\"athletic footwear\", \"sports shoes\",\n",
        "           \"training sneakers\"]\n",
        "# Uses word similarity to match products\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        "### 2. Attention Mechanism Applications\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "\n",
        "#### Document Summarization\n",
        "```text\n",
        "Long text → Attention identifies key sentences\n",
        "→ Generates concise summary focusing on\n",
        "important points\n",
        "```\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "\n",
        "#### Medical Diagnosis\n",
        "```text\n",
        "Patient symptoms → Attention weighs different\n",
        "symptoms → Suggests possible conditions\n",
        "based on learned patterns\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        "### 3. Transformer Architecture in Action\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "#### Code Completion\n",
        "```python\n",
        "def calculate_average(numbers):\n",
        "    total = sum(numbers)\n",
        "    return total/  # Transformer suggests:\n",
        "                  # len(numbers)\n",
        "```\n",
        "\n",
        "#### Legal Document Analysis\n",
        "```text\n",
        "Contract review → Identifies key clauses\n",
        "→ Flags potential issues → Suggests\n",
        "modifications\n",
        "```\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### Content Moderation\n",
        "```text\n",
        "Social media post → Multi-head attention\n",
        "analyzes different aspects:\n",
        "- Hate speech\n",
        "- Inappropriate content\n",
        "- Spam patterns\n",
        "```\n",
        "\n",
        "#### Scientific Research\n",
        "```text\n",
        "Research papers → Analyzes methodology\n",
        "→ Identifies similar studies\n",
        "→ Suggests potential collaborations\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        "### 4. RNN Applications (Historical Context)\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "#### Time Series Prediction\n",
        "```python\n",
        "# Stock price prediction\n",
        "historical_data = [100, 102, 98, 103]\n",
        "predicted_next = rnn.predict(historical_data)\n",
        "# Shows limitations with long sequences\n",
        "```\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### Speech Recognition\n",
        "```text\n",
        "Audio signal → RNN processes sequentially\n",
        "Challenge: Long audio sequences lead to\n",
        "gradient issues\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Modern LLM Applications\n",
        "\n",
        ":::{.columns}\n",
        ":::{.column width=\"50%\"}\n",
        "#### ChatGPT-style Applications\n",
        "```text\n",
        "User: \"Explain quantum computing\"\n",
        "LLM: Processes request using:\n",
        "1. Attention to focus on key concepts\n",
        "2. Contextual understanding\n",
        "3. Knowledge integration\n",
        "```\n",
        "\n",
        "#### Code Generation\n",
        "```python\n",
        "# Comment to code conversion\n",
        "# \"Calculate factorial recursively\"\n",
        "def factorial(n):\n",
        "    if n <= 1: return 1\n",
        "    return n * factorial(n-1)\n",
        "```\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### Educational Tools\n",
        "```text\n",
        "Student: \"I don't understand derivatives\"\n",
        "LLM: Provides:\n",
        "- Step-by-step explanations\n",
        "- Relevant examples\n",
        "- Practice problems\n",
        "```\n",
        "\n",
        "#### Business Applications\n",
        "```text\n",
        "- Email drafting\n",
        "- Report summarization\n",
        "- Data analysis\n",
        "- Customer service automation\n",
        "```\n",
        ":::\n",
        ":::\n",
        "\n",
        ":::{.callout-tip}\n",
        "### Key Takeaway\n",
        "Each advancement in NLP architecture has enabled new applications, with modern LLMs combining the strengths of all previous approaches.\n",
        ":::\n",
        "\n",
        "## Take Aways\n",
        "\n",
        "- Large Language Models (LLMs) use tokenization, word embeddings, attention, and transformer architecture for various text tasks.\n",
        "\n",
        "- Word embeddings represent words in vector space, but contextualized embeddings are needed for better understanding.\n",
        "\n",
        "- Transformers revolutionized NLP with self-attention, enabling efficient long-range processing. BERT (bidirectional) excels at understanding, while GPT (autoregressive) is better at generating text.\n",
        "\n",
        "## References {.smaller .scrollable}\n",
        "\n",
        "- Mikolov, Tomas, et al. \"Efficient Estimation of Word Representations in Vector Space.\" *Proceedings of the International Conference on Learning Representations.* 2013.\n",
        "- Mikolov, Tomas, et al. \"Distributed Representations of Words and Phrases and their Compositionality.\" *Advances in Neural Information Processing Systems.* 2013.\n",
        "- Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate.\" *3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings*. 2015.\n",
        "- Vaswani, Ashish, et al. \"Attention is All You Need.\" *Advances in Neural Information Processing Systems 30 (NIPS 2017)*. 2017."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "/opt/homebrew/anaconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
